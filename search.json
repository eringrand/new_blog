[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "For any businesses interested in hiring a data scientist with over 10 years of work experience, be it for freelance, part time, or full time opportunities, please contact me at eringrand@gmail.com"
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About",
    "section": "Bio",
    "text": "Bio\n\nDegrees:\n\nMS Data Science (Columbia University) 2016\nMS Astronomy (University of Maryland) 2014\nBS Physics & Astronomy (University of Maryland) 2011\n\n\n\nOrganizations\n\nR-Ladies Global and R-Ladies NYC\nNASA Datanauts Top"
  },
  {
    "objectID": "about.html#section",
    "href": "about.html#section",
    "title": "About",
    "section": "",
    "text": "Uncommon Schools (Data Scientist)\n\nProduced high-quality analytics work around student and staff data, including prediction models around student state exams, student enrollment, and staff recruitment.\nLead 6+ group trainings on the R programming language for 10+ coworkers Automated manual state exam reporting process saving 6+ hours of processing time\n\n\n\nCrisis Text Line (Data Scientist)\n\nWorking at Crisis Text Line as a Data Scientist. You can read about my transition to Data Science at Crisis Text Line here.\nBuilt and shipped a new volunteer application which has been used by 7600 applicants. Increased the completion rate from 40% to 90%.\nMy work has focused on identifying drop off points for trainers and supervisors. I produced cohort reports of training metrics and identified key differences among cohorts to provide trainers with new solutions. I identified counselor churn as the largest organizational priority, and setup tracking, reporting, and analysis of key metrics.\n\nTop"
  },
  {
    "objectID": "about.html#research",
    "href": "about.html#research",
    "title": "About",
    "section": "Research",
    "text": "Research\n\nDetermining the formation mechanism for pillar structures found on the rims of HII regions\n\nWorked with Dr.¬†Marc Pound and Dr.¬†Lee Mundy at University of Maryland College Park.\nMy poster from the 2013 CARMA Symposium.\nMaster thesis available at request.\n\nUltraviolet Portrait of M31\n\nCollected images from the ultraviolet telescope aboard NASA‚Äôs swift satellite in order to form the final mosaic.\nWorked with Dr.¬†Stefan Immler at NASA Goddard Space Flight Center.\n\nAnalyzing the Star Formation of Galaxies in the MIDGET survey with Dr.¬†Alberto Bolatto\n\nTop\n\nIn the News\n\npackagemetrics - Helping you choose a package since runconf17\nA Q & A with Erin Grand, Data Scientist at Crisis Text Line\nTamer Center for Social Enterprise Summer Fellowship Interview\nSwift Makes Best-ever Ultraviolet Portrait of Andromeda Galaxy\n\nTop"
  },
  {
    "objectID": "posts/new_blog_who_dis/index.html",
    "href": "posts/new_blog_who_dis/index.html",
    "title": "New Blog - Who Dis?",
    "section": "",
    "text": "I started my blogging journey as a homework assignment in grad school. At the time, Jekyll was new, exciting, and easy to spin up. I found a theme I liked, contributed to the theme on GitHub so it had all the elements I needed, got an internship from the person who ran the theme (Thanks Barry!), and stuck with it till now.\nThere‚Äôs nothing wrong with my (old?) blog, but I‚Äôve been using Quarto docs in my work-life and wanted to experiment with Quarto powering my blog. I don‚Äôt write a new post very often, so I also don‚Äôt have that many posts to transition. It seemed like a fun challenge!"
  },
  {
    "objectID": "posts/new_blog_who_dis/index.html#task-1-get-old-blog-file-structure-into-new-file-structure",
    "href": "posts/new_blog_who_dis/index.html#task-1-get-old-blog-file-structure-into-new-file-structure",
    "title": "New Blog - Who Dis?",
    "section": "Task 1: Get old blog file structure into new file structure",
    "text": "Task 1: Get old blog file structure into new file structure\nA Quarto blogs file tree look like:\n‚îú‚îÄ‚îÄ 404.html\n‚îú‚îÄ‚îÄ 404.jpg\n‚îú‚îÄ‚îÄ CNAME\n‚îú‚îÄ‚îÄ _quarto.yml\n‚îú‚îÄ‚îÄ about.qmd\n‚îú‚îÄ‚îÄ index.qmd\n‚îú‚îÄ‚îÄ posts\n‚îÇ   ‚îú‚îÄ‚îÄ _metadata.yml\n‚îÇ   ‚îú‚îÄ‚îÄ my_first_post\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.qmd\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stockphoto.png\n‚îú‚îÄ‚îÄ profile.png\n‚îî‚îÄ‚îÄ styles.css\nwith each post having it‚Äôs own sub-folder under the posts directory. The text of each post is inside an index.qmd file, which contains date and tag metadata in the yaml.\nOn the other hand, my Jekyll blog directory looks like has markdown files under the _posts folder.\n  ‚îú‚îÄ‚îÄ 404.md\n  ‚îú‚îÄ‚îÄ CNAME\n  ‚îú‚îÄ‚îÄ _config.yml\n  ‚îú‚îÄ‚îÄ _includes\n  ‚îú‚îÄ‚îÄ _layouts\n  ‚îú‚îÄ‚îÄ _plugins\n  ‚îú‚îÄ‚îÄ _posts\n  ‚îÇ   ‚îú‚îÄ‚îÄ 2015-02-15-my_first_blog.md\n  ‚îú‚îÄ‚îÄ _sass\n  ‚îú‚îÄ‚îÄ about.md\n  ‚îú‚îÄ‚îÄ archive.html\n  ‚îú‚îÄ‚îÄ feed.xml\n  ‚îú‚îÄ‚îÄ htmlwidgets_deps\n  ‚îú‚îÄ‚îÄ images\n  ‚îÇ   ‚îú‚îÄ‚îÄ 404.jpg\n  ‚îÇ   ‚îî‚îÄ‚îÄ stockphoto.png\n  ‚îú‚îÄ‚îÄ index.html\n  ‚îú‚îÄ‚îÄ projects.md\n  ‚îú‚îÄ‚îÄ style.scss\n  ‚îú‚îÄ‚îÄ tag_index.html\n  ‚îî‚îÄ‚îÄ teaching.md\nSo the first task is to convert all my markdown blog posts into their own sub-folder under a posts directory. I would also like to grab any and all meta data out of the posts while doing so.\nlibrary(tidyverse)\n\n# md posts filenames & location\nposts &lt;- list.files(\"eringrand.github.io.raw/_posts/\", full.names = TRUE)\n\nposts_raw &lt;- data.frame(post_loc  = posts, # full filepath\n                        post_name = basename(posts) # just the file name\n                        ) |&gt; \n  rowwise() |&gt; \n  # use list-frame to read in post text\n  mutate(post = list(data.frame(txt = readLines(post_loc))))\n\n# post meta data\nposts_info &lt;- posts_raw |&gt; \n  mutate(date = str_sub(post_name, 1, 10), # Jekyll post filenames all start with the date\n         author = \"Erin Grand\" \n         ) |&gt; \n  # grab metadata from the text of the post itself\n  mutate(title = filter(post, str_detect(txt, \"title:\")) |&gt; pull(txt), \n         title = str_remove(title, \"title:\") |&gt; \n           str_remove_all(\"[[:punct:]]\") |&gt; \n           str_trim(),\n         # this may be different depending on how your blog does tags\n         categories = filter(post, str_detect(txt, \"tags:\")) |&gt; \n                             pull(txt) |&gt; \n                             str_remove(\"tags:\") |&gt; \n                             str_remove_all(\"[[:punct:]]\") |&gt; \n                             str_trim() |&gt; \n                             str_c(collapse = \", \"),\n         ) \nI then did a bunch of text cleaning of my tags/categories, but honestly I ended up rewriting them manually anyway, so I‚Äôm going to ignore that code for now.\nWith all the post information, and most of the meta data, now I could write the new post structure and files.\nfill_between &lt;- function(x) {\n  x_log = str_detect(x, \"---\")\n  \n  # Find the indices of the first and last TRUE\n  first_true_index &lt;- which(x_log)[1]\n  last_true_index &lt;- which(x_log)[2]\n\n  # Create a range of indices to fill\n  indices_to_fill &lt;- first_true_index:last_true_index\n\n  # Set all values within this range to TRUE\n  return(x[-indices_to_fill])\n}\n\nposts_all &lt;- posts_info |&gt; \n  mutate(post_txt = list(data.frame(x = fill_between(pull(post, txt)))), # continue to use list-frames\n         # new yaml headers, with date and tags\n         yml_txt = (con = glue::glue(\"---\n                                     title: {title}\n                                     author: {author}\n                                     date: {date}\n                                     categories: [{categories}]\n                                     image: 'image.jpg'\n                                     ---\")\n                    ),\n         yml_txt = list(data.frame(readLines(textConnection(yml_txt)))),\n         new_post_txt = list(bind_rows(yml_txt, post_txt) |&gt; pull(x)),\n         dir_title = janitor::make_clean_names(title)\n         ) |&gt; \n  select(dir_title, new_post_txt)\nNow with the posts written in the way I wanted them, I just had to create the new sub-folders, and write the posts out into individual index.qmd files.\nwalk(pull(posts_all, dir_title), ~dir.create(glue::glue(\"posts/{.x}\")))\nwalk2(pull(posts_all, dir_title), pull(posts_all, new_post_txt), ~writeLines(.y, glue::glue(\"posts/{.x}/index.qmd\")))"
  },
  {
    "objectID": "posts/new_blog_who_dis/index.html#task-2-render-the-blog-and-fix-all-the-errors",
    "href": "posts/new_blog_who_dis/index.html#task-2-render-the-blog-and-fix-all-the-errors",
    "title": "New Blog - Who Dis?",
    "section": "Task 2: Render the Blog and Fix all the Errors",
    "text": "Task 2: Render the Blog and Fix all the Errors\nA significant number of my posts stopped working due to links that didn‚Äôt go anywhere, and images that I didn‚Äôt grab from the original posts. (Whoops! Next time, automate image file moving as well.) I didn‚Äôt have that many blog posts, so I it was not a huge lift for me to manually test each post and edit the text to have correct image links (where possible).\nI also went in and changed most of the categories because my old tags didn‚Äôt make as much sense to me anymore. (It‚Äôs an R blog! Does every post need a #tidyverse tag? Probably not.)\nThis left with a functional blog!"
  },
  {
    "objectID": "posts/new_blog_who_dis/index.html#task-3-make-blog-pretty",
    "href": "posts/new_blog_who_dis/index.html#task-3-make-blog-pretty",
    "title": "New Blog - Who Dis?",
    "section": "Task 3: Make Blog Pretty!",
    "text": "Task 3: Make Blog Pretty!\nErin, you ask, this blog doesn‚Äôt any different from the default theme? Did you finish the new blog??? Dear reader, no, I have not. I‚Äôm trying out some themes, and taking a stab at css and brand.yml, but none of those things are ready for Prime Time yet."
  },
  {
    "objectID": "posts/data_science_jobs_in_the_nonprofit_sector/index.html",
    "href": "posts/data_science_jobs_in_the_nonprofit_sector/index.html",
    "title": "Data science jobs in the not-for-profit sector",
    "section": "",
    "text": "I‚Äôve been a data scientist in the non-profit sector for 5+ years. In an earlier conversation with people starting their transition into data science, we talked about how non-profit data can be an excellent place to start. There are some cons to beginning at a non-profit - but I love it.\nIn my experience, non-profits data roles have similarities with start-up positions. A non-profit data role may involve doing more work than described in the job description (e.g., analysis and project management at the same time). Due to a lack of funding, non-profits cannot hire individual people for all the needed work, so a new hire can end up filling many job roles at once. This early experience in more extensive work is a magnificent step for learning on the job. ‚ÄúPersonally, I think non-profit DS is the perfect place to start a career :) so much messy data, and so many wins, even from‚Äùbasic‚Äù models.‚Äù (Caitlin Hudon) In larger for profit-tech companies, In for-profit tech companies, people are not wearing some many hats. In non-profits, you end up being more cross trained.\nOn the other hand, a possible con to joining a smaller non-profit OR start-up is that you may be one of the first data people in the company. Data Science teams often don‚Äôt exist at non-profits, and as the first or early data person, you are forced to grow your skills quickly on the job, but it also means you won‚Äôt have an on-site mentor on the job. ‚ÄúIn my current position, my manager is self-taught in coding‚Ä¶ He does most things in SQL.‚Äù - Kevin Gilds. In chapter 9 of their book, Building a Career in Data Science, Emily Robinson & Jaqueline Nolis talk more about being the first or only data person on the job. When there is a data team in place, non-profits often have smaller data teams. The day-to-day work is likely to be more aligned with reporting/analysis than machine learning. If you are looking specifically to get involved in modeling ASAP, a non-profit will not be the best place to start. That said, the messy data difficulties in a non-profit can often lead to quick wins for automation and coding.\nIn companies without a dedicated data team, data structures and cleaning become crucial as data is likely in spreadsheets. SQL and Excel skills are more appreciated than complicated programming skills. I have loved working with messy data as it has allowed me to shape the policies and work to create more organized structures. ‚ÄúYou will never find better data sets to cut your teeth on than working with non-profit data.‚Äù (Caitlin Hudon) Given the often early data stages, you can do quick magic with a spreadsheet. Apply some automation in an excel macro or a pivot table gives you an early win with the data and stakeholders. Teaching others how to do a vlookup could make a difference between them spending 5min vs.¬†1-2 hours on a task.\nThe clear win or difference, for me, at a non-profit is that everyone is passionately mission-driven. Non-profit employees are not there just for the money (though, hey - let‚Äôs pay people what they‚Äôre worth, please). Tech and data skills can be uniquely used in a mission-driven space to do fantastic work to make the world better. For example, in an education company, working with student data can directly influence how a teacher can help them perform even better on SATs, enabling them access to a better college. At a mental health company, data work provides information for counselors to better help their clients - literally saving lives."
  },
  {
    "objectID": "posts/new_york_rstats_conference/index.html",
    "href": "posts/new_york_rstats_conference/index.html",
    "title": "New York Rstats Conference",
    "section": "",
    "text": "Last weekend was the 2017 RStatsNYC conference. I had a great time talking to friends and meeting new friends throughout the weekend. The speakers covered a variety of topics from data ethics to cloud cloud computing. I‚Äôve complied my notes, plus some of popular tweets from the conference below.\n\nDay 1\n\nHow R Helps Airbnb Make the Most of Its Data\nRicardo Bion, Airbnb\n\nAirBnD started in 2008 with 1 city and 1 room, now there are 3M homes in 71K cities\n100+ data scietists using a mix of lanaguages mostly R, but lots of python\nWhy to use an R Packages:\n\nPassing around functions required duplication of work, where as a package can include data, test, add-ins, vignettes, R markdown and notebook templates\nThe AirBnB packages have consistent API, branded visualization, branded templates, and of course function functions\n\nEducation:\n\nmade a difference in confidence of R stats\nnew hire buddy\nintro classes at datacamp if interested, sponsored by airbnb\npeer support with office hours, code review, R slack group\nlearning lunch, journal club, offsites\n\nReproducibility:\n\nscale knowedlge\nknowledge repo\nposts have tags with topics, date, then served as a web ui\nuses github for peer review\nbranded template\nincorprate best practices from academia and software\n\n\n\n\nHearing about reproducibility and R packages at Airbnb from @ricardobion at NYR #rstatsnyc pic.twitter.com/uGSZwtX1XM\n\n‚Äî Julia Silge (@juliasilge) April 21, 2017\n\n\n\n\nFine Grained Visual Category Recognition and Perceptual Embedding\nSerge Belongie, Cornell University\n\nReally intersting talk on using Stochastic Neighbor algorithm with crowd sourcing to get a visual similarity of images.\nMotivation of humans and computers working together\nHis talk focused on detecting what type of bird was in an image\nDetecting that there is a bird in the picture is getting easy for computer, but detecting the exact name of the bird is much more difficult\n\n\n\n.@SergeBelongie showing a stochastic neighbor algorithm with crowd sourcing to get a visual similarity of images #rstatsnyc pic.twitter.com/TRuIruTELS\n\n‚Äî Erin Grand (@astroeringrand) April 21, 2017\n\n\n\n\nAn R Cloud Computing Lifeline: The Missing Manual for Running R on Amazon Cloud\nKelly O‚ÄôBriant, B23\n\nWorking with R in the cloud is different from working with Rstudio on your computer, you have to install all your favroite packages again every time you start up a new server\nbigger instances sizes, analysis while sleeping, running multiple R servers at the same time, instances themselves are disaposable and renewable resources\nable to use tools in a more powerful manor\n\n\n\n@b23kelly created a custom R package to set up new projects/deployed servers fast without having to reconfigure anew each time #rstatsnyc\n\n‚Äî Alec Barrett (@alecbarrett) April 21, 2017\n\n\n\n\nR Makes the World Go ‚ÄôRound: Data-Driven Decision Making at JetBlue\nCatherine Zhou, JetBlue\nOne of the interesting take always throughout the conference was that you can start coding in R pretty quickly, if you start with the right ideas and tools about what R is. Most intro stats classes (mine included) treat R and other programming languages as a calculator. But R is so much more than that! Catherine Z made a point about giving people templates with tidyverse functions to produce their own analyses. In my own work, I‚Äôm helping my coworker think of tidyverse in a similar way to computing excel tasks, i.e group_by() %&gt;% summarise() is equivalent to a pivot table, and mutate() adds a new column the same you way you might by drag-and-dropping an excel equation.\nThere were also several good comments on how you can learn slowly by doing something small in R (such as doing a bit of cleaning) and then porting it back out to excel or tableau to finish your analysis. Best takeaway? Not everyone needs to be fluent in R. - Catherine\n\n\n.@catherinezh Easy sells: Automation and reproducibility, but not everyone needs to be fluent. pic.twitter.com/EancCU2eWb\n\n‚Äî Erin Grand (@astroeringrand) April 21, 2017\n\n\n\n\nTheoretical Statistics is the Theory of Applied Statistics: How to Think About What We Do\nAndrew Gelman, Columbia\nReiterating Andrew Gelman‚Äôs point about how p-value statistical testing is actually a pretty bad framework for hypothesis testing most of the time. time to really brush up on Bayesian stats. I also liked the general point that you should have to state what you expect to find (alternate hypotheses, estimated effect sizes, whatever) before you go barging around looking for anything and act like whatever you end up with is what you were searching for all along.\n\n\nThe Unreasonable Effectiveness of Empathy - The killer skill needed for a successful technical career\nJD Long, RenaissanceRe\n\nAnalysis doesn‚Äôt end at result delivery - it ends at developing and proselytizing new business strategies and innovation.\nAutomating Excel workflows can be a first step towards bringing R to a team.\nAgile development tells user stories as an empathy hack.\n\nAs a ______\nI want ______\nSo I can ______ \n\nPerson-level stories (the near) are always more meaningful than data stories (the far). We need to balance both as Data Scientists.\nIn development, we need to have an actual user in mind, rather than a theoretical user who wants everything.\n\n\n\n‚ÄúAs you tell your data stories, think about the individual people in your data and your consumers‚Äù - @CMastication #rstatsnyc pic.twitter.com/f9DZGCprDO\n\n‚Äî Emily Robinson (@robinson_es) April 21, 2017\n\n\n\n\n\nDay 2\n\nUsing Human Mobility Data to Assess Public Circulation Health\nMichael Kane\n\ntalking about cell phone tower data -&gt; homan ciculatory data\nanytime you are connecting or handing off to a cell phone tower, the data is logged\nsecruity at: IDS are all hashed version of cell phones\n35 TB data set! big! 10 GB every day\ntouching ALL of the day when doing analysis\nuse R, with Hadoop\nusing HMR in cluster (hapood)\nlooks at inflow and outflow counts (flux)\nare providing reliable information on human mobility on storms\nwant the learner that is the most REGULAR not the closest to accuracy\n\n\n\nFrom Agreeing to Marching to Organizing: OSS Needs You\nMike Malecki and Neal Richardson\n\nBest ways to contribute to open source are to start with improving documentation\nOpen source contributions: failing test with fix &gt; failing test &gt; bug report\nRemember to include sessionInfo()\nWhen releasing a package, release quickly, but also slowly - take time to fix dumb decisions\n\nBring something new to the community, but don‚Äôt reinvent the wheel\nTell people about your package (social media), then listen to how they‚Äôre using it\nWhen thinking about a package: documentation &gt; usability &gt; performance &gt; features\n\n\n\n\n\nOther Learnings\nI spent most of the conference chatting and meeting other members of the Rladies New York chapter.\n\n\n\nPackages to Try Out\n\nRXKCD: add XKCD cartoons to stuff\ntrelliscope: many-panel data vis\ncompareGroups: compare demographics and other aspects across groups\ngoodpractice: does a variety of checking for good package development practice\nlintr: helps check for good code style"
  },
  {
    "objectID": "posts/faces_of_rstudioconf/index.html",
    "href": "posts/faces_of_rstudioconf/index.html",
    "title": "Faces of rstudioconf",
    "section": "",
    "text": "I was reminded today by Daniela that everyone should blog - and on top of that you can totally blog something small and simple just to get something out there.\nIn the spirit of small and simple, last week I tweeted out this cool image‚Ä¶\n\n\nI am writing up my notes from #rstduiconf (blog post coming!) and wanted to have a quick picture to go with my thoughts. Remembering @ma_salmon ‚Äôs post on Faces of R (here: https://t.co/C1sRW3hwVL ), I decided to make a Faces of Rstudioconf! pic.twitter.com/vtNVn2RyHV\n\n‚Äî Erin Grand (@astroeringrand) February 8, 2018\n\n\nI have not yet organized all my thoughts from the conference (spoilers, it was awesome, I learned so much!), but that will not stop by from posting the code I borrowed from Maelle to create the pretty image. So, here you go!\nlibrary(rtweet)\nlibrary(magick)\nlibrary(tidyverse)\nlibrary(gmp)\nsearch_terms &lt;- c(\"rstudioconf\", \"rstudioconf2018\")\n\ntweets &lt;- purrr::map_df(search_terms, search_tweets, n=2000, include_rts=FALSE, parse=TRUE) \n\nusers_tweets &lt;- lookup_users(unique(tweets$user_id))\n\nusers &lt;- users_tweets %&gt;%\n  select(user_id, \n         profile_image_url, \n         screen_name,\n         name, \n         followers_count, \n         profile_image_url\n         ) %&gt;%\n  distinct()\n\nsave_image &lt;- function(df){\n  image &lt;- try(image_read(df$profile_image_url), silent = FALSE)\n  if(class(image)[1] != \"try-error\"){\n    image %&gt;%\n      image_scale(\"50x50\") %&gt;%\n      image_write(paste0(\"~pictures/\", df$screen_name,\".jpg\"))\n  }\n}\n\nusers &lt;- filter(users, !is.na(profile_image_url))\nusers_list &lt;- split(users, 1:nrow(users))\nwalk(users_list, save_image)\n\n\nfiles &lt;- dir(\"pictures/\", full.names = TRUE)\nset.seed(42)\nfiles &lt;- sample(files, length(files))\ngmp::factorize(length(files))\n\nno_rows &lt;- 14\nno_cols &lt;- 31\n\nmake_column &lt;- function(i, files, no_rows){\n  image_read(files[(i*no_rows+1):((i+1)*no_rows)]) %&gt;%\n  image_append(stack = TRUE) %&gt;%\n    image_write(paste0(\"cols/\", i, \".jpg\"))\n}\n\nwalk(0:(no_cols-1), make_column, files = files, no_rows = no_rows)\n\nimage_read(dir(\"cols/\", full.names = TRUE)) %&gt;%\nimage_append(stack = FALSE) %&gt;%\n  image_write(\"2018-02-7-facesofrstudioconf.jpg\")"
  },
  {
    "objectID": "posts/reviewing_my_old_live_journal_posts/index.html",
    "href": "posts/reviewing_my_old_live_journal_posts/index.html",
    "title": "Reviewing my old live journal posts",
    "section": "",
    "text": "I‚Äôm about to BARE MY SOUL to the internet. Well, the soul of my teenage self. Get ready!\nLive Journal, you may remember, was(/is - it does still exist) was a blogging site before we really knew what blogging was. It was both a place to put diary entries and those quizzes that got passed around in the day. It was also used for FanFic and community gathering. I did NOT use my (main) account for fan gathering. (Though I did write some excellent/terrible fanfics. Didn‚Äôt everyone have a Lord of Rings self-insert character?)\nI don‚Äôt really remember when I started this idea, but I thought it would be fun to see just how emo I was in 2006. Let‚Äôs go!"
  },
  {
    "objectID": "posts/reviewing_my_old_live_journal_posts/index.html#step-0-load-the-libraries",
    "href": "posts/reviewing_my_old_live_journal_posts/index.html#step-0-load-the-libraries",
    "title": "Reviewing my old live journal posts",
    "section": "Step 0: Load the libraries",
    "text": "Step 0: Load the libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tidytext)\nlibrary(hunspell)\nlibrary(ggrepel)\nlibrary(cowplot)"
  },
  {
    "objectID": "posts/reviewing_my_old_live_journal_posts/index.html#step-1-clean-the-data",
    "href": "posts/reviewing_my_old_live_journal_posts/index.html#step-1-clean-the-data",
    "title": "Reviewing my old live journal posts",
    "section": "Step 1: Clean the data",
    "text": "Step 1: Clean the data\nI downloaded all my past Live Journal entities to a folder on my desktop in the same CSV format, so that I could easily load them in for analysis. I am pleasantly surprised that Live Journal made it so easy to download my history like this! I did have to click the same button a ton of time - but I did get all my data.\nThe next step was to take every journal and separate out the individual words using ‚Äòtidytext.‚Äô\nlj_words &lt;- lj_df %&gt;%\n  select(itemid, eventtime, logtime, subject, current_music, current_mood, event) %&gt;%\n  mutate(across(c(eventtime, logtime), ymd_hms),\n         year = year(logtime),\n         month = month(logtime),\n         ) %&gt;%\n  mutate(event = str_remove_all(event, \"'\")) %&gt;%\n  unnest_tokens(word, event, token = \"words\", format = \"html\", strip_url = FALSE) \nNot included in this blog post, for privacy of my teenage friends, I also cleaned and changed names of my friends and locations to clean the data and protect their privacy. For example, instead of the name ‚ÄúLinda‚Äù you may see ‚Äúnameofsister‚Äù.\nI was (and continue to be) terrible at spelling words correctly and also terrible at checking what I‚Äôve typed after the fact. I use ‚ÄòHunspell‚Äô here in an attempt to fix some of the most common issues. Does this spell check get everything? No! But alas, I am a terrible speller and we move on in life.\nlj_words_spell_check &lt;- lj_words_protect %&gt;%\n  anti_join(my_stop_words, by = \"word\") %&gt;%\n  count(word) %&gt;%\n  rowwise() %&gt;%\n  mutate(spell_check = hunspell(word)) %&gt;%\n  filter(length(spell_check) &gt;= 1) %&gt;%\n  mutate(suggest = hunspell_suggest(spell_check)) \nlj_correct &lt;- lj_words_spell_check %&gt;%\n  filter(length(suggest) &gt; 0) %&gt;%\n  mutate(suggest_pick = pluck(suggest, 1)) %&gt;% # just pick the first one because I am lazy\n  ungroup() %&gt;%\n  unnest(suggest_pick) %&gt;%\n  select(word, suggest_pick) \nlj_words_corrected &lt;- lj_words_protect %&gt;%\n  left_join(lj_correct, by = \"word\") %&gt;%\n  mutate(word = coalesce(suggest_pick, word)) %&gt;%\n  unnest_tokens(output = \"word\", input = word) # used because sometimes the correction is actually 2+ words now"
  },
  {
    "objectID": "posts/reviewing_my_old_live_journal_posts/index.html#step-2-now-we-move-on-to-analysis",
    "href": "posts/reviewing_my_old_live_journal_posts/index.html#step-2-now-we-move-on-to-analysis",
    "title": "Reviewing my old live journal posts",
    "section": "Step 2: Now we move on to analysis!",
    "text": "Step 2: Now we move on to analysis!\nThe data is clean, or at least as clean as it is going to get today.\n\nWord counts\nI start with TF-IDF. The goal here is to see what I was talking about each year and how it may differ as I got older. As a reminder, I have changed the names of all my friends and family for privacy. That way you don‚Äôt know who ‚Äúnameofbestfriend‚Äù is and why I stopped mentioning ‚Äúnameofbestfriend‚Äù in 2006. (We had a bit of a falling out at the end of HS.)\ntfidf &lt;- lj_words_corrected %&gt;%\n  count(year, word) %&gt;% \n  tidytext::bind_tf_idf(word, year, n) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  group_by(year) %&gt;%\n  top_n(n = 10, wt = tf_idf) %&gt;%\n  ungroup() %&gt;%\n  filter(n &gt;= 2) \n\nLook at 2009 - clearly my only entries were my Norse myth college class. I remember I put a few of my class papers on my Live Journal.\nWe can look at the differences between TF-IDF and a regular word count, while accounting for stop words.\nwordcount &lt;- lj_words_corrected %&gt;%\n  count(year, word) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  group_by(year) %&gt;%\n  top_n(n = 10, wt = n) %&gt;%\n  ungroup() %&gt;%\n  filter(n &gt;= 3) \n\n\n\nSentiment\nNext I look at sentiment. I remember using live journal to be super angsty. I assumed that I would largely see negative sentiment and words across the years.\ndf_plot &lt;- lj_words_corrected %&gt;%\n  left_join (get_sentiments(\"bing\"), by = \"word\" ) %&gt;%\n  rename(bing_sentiment = sentiment) %&gt;%\n  left_join (get_sentiments(\"nrc\"), by = \"word\" ) %&gt;%\n  rename(nrc_sentiment = sentiment) %&gt;%\n  pivot_longer(cols = c(bing_sentiment, nrc_sentiment), \n               names_to = \"sentiment_type\", values_to = \"sentiment\") %&gt;%\n  count(sentiment_type, year, sentiment) %&gt;%\n  filter(! is.na(sentiment) ) %&gt;%\n  rename(count = n ) %&gt;%\n  group_by(sentiment_type, year) %&gt;%\n  mutate(total = sum(count)) %&gt;%\n  ungroup() %&gt;%\n  mutate(percent = count / total,\n         year_month = ymd(str_c(year, \"01\", \"01\", sep = \"-\"))\n         ) \nInstead, it seems my words were largely more positive then negative. (Outside of 2009 - which is either largely from my anxiety attacks that year or Norse mythology is just super depressing.) Not as ansgty as I remember!\n\nAh, but did I mark my ‚Äúcurrent mood‚Äù / how are you feeling‚Äù part as positive as my words are? You be the judge."
  },
  {
    "objectID": "posts/reviewing_my_old_live_journal_posts/index.html#end",
    "href": "posts/reviewing_my_old_live_journal_posts/index.html#end",
    "title": "Reviewing my old live journal posts",
    "section": "End :)",
    "text": "End :)\nSo there you have it. Was teenage Erin as emo as I thought? Maybe not! Or maybe I wrote all the most emo journals in my physical diary. The world will never know (because those diaries have been lost)."
  },
  {
    "objectID": "posts/r_in_the_world_of_education/index.html",
    "href": "posts/r_in_the_world_of_education/index.html",
    "title": "R in the World of Education",
    "section": "",
    "text": "I recently gave a r-ladies presentation about my work cleaning and working with really messy education data. This blog post is an attempt at summarizing the main points of the talk.\nLook how cool I am. Go me! The slides are here in case you missed them."
  },
  {
    "objectID": "posts/r_in_the_world_of_education/index.html#what-kind-of-data-do-we-work-with",
    "href": "posts/r_in_the_world_of_education/index.html#what-kind-of-data-do-we-work-with",
    "title": "R in the World of Education",
    "section": "What kind of data do we work with?",
    "text": "What kind of data do we work with?\nThe Data Analytics team focuses at the overall picture, so while we don‚Äôt work on reporting data directly back to the state (our regional teams have excellent people working on this) we do get to work with most of the data the organization collects.\nThe data we work with generally fits into one of these buckets‚Ä¶\n\nAssessments: Interim assessments are taken as practice tests through out the school year.\nExams: Common Core aligned state exams, SAT, PSAT, APs, ‚Ä¶etc\nClassroom: Assignment grades, attendance, suspensions, ‚Ä¶etc\nTeacher: student - course - teacher linkage information\nStaff Data: HR and Recruitment\n\nUnfortunately, given the amount of data we have and the number of sources it may be coming from, we have a ton of data challenges to overcome in every analysis.\nOf course, every piece of data has its own challenges and ‚Äúmessy nature,‚Äù but there are patterns.\n\nMissing/Incomplete data\nDifferent data sources without matching IDs (i.e HR to Teacher to Student)\nMovement between schools and courses of students and teachers\nAlignment of data and data processes across all schools and regions\nStudent IDs that change\nHuman data reporting error\nHistorical data quality\n\nSome of these challenges are easy to fix and some are harder. For example, a messy excel sheet can be cleaned (by hand or by code). We‚Äôve developed (or are developing in some cases) systems to work with most of these types of challenges.\n\nMessy excel sheets (historical or human entered)\nColumn names that don‚Äôt apply anymore\nLack of historical documentation\nFinding duplicate tests\nStudents that take half of one test and the other half of another\nVanishing leading zeros\nTracking of student IDs that change\nLack of common definitions (i.e ‚Äúcohort‚Äù)\nHow to refer to school years or school abbreviations\nData audits"
  },
  {
    "objectID": "posts/r_in_the_world_of_education/index.html#an-example-using-janitor-to-clean-a-messy-excel-file.",
    "href": "posts/r_in_the_world_of_education/index.html#an-example-using-janitor-to-clean-a-messy-excel-file.",
    "title": "R in the World of Education",
    "section": "An example, using janitor to clean a messy excel file.",
    "text": "An example, using janitor to clean a messy excel file.\nI‚Äôm often tasked with cleaning roster files, which contain entry and exit data for students. These files can be very messy due to students who moved between schools or were not exited properly from the system, causing duplicates.\nTo clean this data, first I read it in with read_excel and use janitor's clean_names to convert all the column names to something I can use. remove_empty() removes entire columns or rows that are NA as excel sometimes can‚Äôt tell where there is data and where there isn‚Äôt.\nI choose to use col_type = \"text\" in my read_excel statement, because I sometimes have to deal with leading zeros, NAs that are not written as NAs, or other text fields in my numerical columns. Reading in as text and converting later allows me to find and correct problems before they become NAs. I use mutate_at to convert these columns back to numbers after examining that everything looks good.\nstudents &lt;- readxl::read_excel(filepath, sheet=\"Sheet1\", col_types = \"text\") %&gt;%\n  janitor::clean_names() %&gt;%\n  janitor::remove_empty_cols() %&gt;%\n  janitor::remove_empty_rows() %&gt;%\n  dplyr::mutate_at(vars(entrydate, exitdate, student_id, yearsinuncommon), as.numeric) %&gt;%\n  dplyr::mutate_at(vars(entrydate, exitdate), excel_numeric_to_date) \nThe next step in data cleaning is to look for duplicates. Luckily, janitor has a super helpful get_dupes() function which does just that!\nstudents %&gt;% \n  get_dupes(student_id)\n# A tibble: 2 x 6\n  student_id dupe_count grade yearsinuncommon  entrydate   exitdate\n       &lt;dbl&gt;      &lt;int&gt; &lt;dbl&gt;           &lt;dbl&gt;     &lt;date&gt;     &lt;date&gt;\n1    2342675          2    10               1 2017-11-11 2017-12-11\n2    2342675          2    11               1 2017-11-11 2017-12-11\nIn this example data, I have one student with duplicate information. They‚Äôre in two different grades, ugh! Now, I have to choose a method to correct this student in my data.\n\nThere are three main ways I use to correct dupes.\n\n1. Correct the dupes individually with if_else or case_when.\nIf there are only a few duplicates, or they‚Äôre all in one grade or class room, a quick set of if statements will do the trick to make the rows perfectly duplicated. From there, use distinct to get only distinct rows and remove the duplicates.\nmutate(students, grade = if_else(student_id == 2342675, 10, grade))\n\n\n2. Summarize by taking minimum date / grade to choose one row to keep.\nThis is helpful if you just need one of the rows, and don‚Äôt really care which row is the one you keep. For example, our exit and enter date information is not usually great, so I‚Äôm okay with the ‚Äòjust pick one‚Äô version as long as the student‚Äôs grade and teacher information is the same in both rows.\ngroup_by(students, student_id) %&gt;% summarize(grade = min(grade))\n\n\n3. Output the duplicates and manually choose which version to keep.\nThis involves the most manual work, so I usually grab help when I need to do this, yay teammates!\ndupes_correct &lt;- read_csv(\"dupes_correct.csv\")\nleft_join(students, dupes_correct) %&gt;%\n  replace_na(list(keep = 0)) %&gt;%\n  assert(not_na, keep) %&gt;%\n  filter(keep = 0)\n\n\n\nManaging Data Changes\nAs data is updated, there might be more duplicates to worry about. A great way to check for duplicate updates is to use the 1:2 punch of janitor‚Äôs get_dupes and assertr‚Äôs verify(). This allows you to put checks in place in case the data changes.\ncheck &lt;- students %&gt;% \n  get_dupes(student_id) %&gt;% \n  verify(nrow(.) == 0)\nIf new duplicates occur the code will HALT at this step alerting that something is wrong."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erin Grand",
    "section": "",
    "text": "New Blog - Who Dis?\n\n\n\nblog\n\nquarto\n\n\n\n\n\n\n\n\n\nDec 5, 2025\n\n\nErin Grand\n\n\n\n\n\n\n\n\n\n\n\n\nBest Practices for Cleaning Data in R\n\n\n\ndata science\n\nR\n\npresentation\n\n\n\n\n\n\n\n\n\nNov 14, 2025\n\n\nErin Grand\n\n\n\n\n\n\n\n\n\n\n\n\nReviewing my old live journal posts\n\n\n\nsentiment\n\ntidytext\n\nR\n\n\n\n\n\n\n\n\n\nJun 28, 2022\n\n\nErin Grand\n\n\n\n\n\n\n\n\n\n\n\n\nData science jobs in the not-for-profit sector\n\n\n\ndata science\n\ncareers\n\n\n\n\n\n\n\n\n\nApr 14, 2021\n\n\nErin Grand\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter‚Äôs Favorite Lesser Known Packages\n\n\n\nR\n\ntwitter\n\n\n\n\n\n\n\n\n\nJun 30, 2020\n\n\nErin Grand\n\n\n\n\n\n\n\n\n\n\n\n\nScraping APOD Descriptions\n\n\n\ntidytext\n\n\n\n\n\n\n\n\n\nApr 21, 2018\n\n\nErin Grand\n\n\n\n\n\n\n\n\n\n\n\n\nFaces of rstudioconf\n\n\n\nR\n\ntwitter\n\nconference\n\n\n\n\n\n\n\n\n\nFeb 15, 2018\n\n\nErin Grand\n\n\n\n\n\n\n\n\n\n\n\n\nR in the World of Education\n\n\n\nR\n\ndata science\n\neducation\n\nrladies\n\n\n\n\n\n\n\n\n\nDec 30, 2017\n\n\nErin Grand\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate to Lizzie Bennet Text Analysis Using Plotly\n\n\n\nR\n\nsentiment\n\ntidytext\n\nLBD\n\nplotly\n\nrladies\n\n\n\n\n\n\n\n\n\nMay 31, 2017\n\n\nErin Grand\n\n\n\n\n\n\n\n\n\n\n\n\nText Analysis of The Lizzie Bennet Diaries\n\n\n\nR\n\nrladies\n\nR\n\ntidytext\n\nLBD\n\n\n\n\n\n\n\n\n\nMay 2, 2017\n\n\nErin Grand\n\n\n\n\n\n\n\n\n\n\n\n\nNew York Rstats Conference\n\n\n\nconference\n\nR\n\nrladies\n\nR\n\n\n\n\n\n\n\n\n\nApr 30, 2017\n\n\nErin Grand\n\n\n\n\n\n\n\n\n\n\n\n\nGraphics in Science\n\n\n\nvisualization\n\ndata science\n\npython\n\nR\n\n\n\n\n\n\n\n\n\nMar 24, 2015\n\n\nErin Grand\n\n\n\n\n\n\n\n\n\n\n\n\nEducational Graphics in Science\n\n\n\n\n\n\n\n\nMar 24, 2015\n\n\nErin Grand\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/twitters_favorite_lesser_known_packages/index.html",
    "href": "posts/twitters_favorite_lesser_known_packages/index.html",
    "title": "Twitter‚Äôs Favorite Lesser Known Packages",
    "section": "",
    "text": "At the 2018 December NYC R Ladies meetup (yes this post has been sitting in my drafts for over a year), a group started talking about how a few tiny functions in a lesser-known package can provide you with serious magic. The problem is finding those packages and functions! With so many amazing packages on CRAN and GitHub, how do you even begin to search? One way - ask all your twitter followers what they think, and twitter did not disappoint - so here are some examples of amazing packages and functions you might want to learn about.\nThe types of functions offered seemed to fall in a couple buckets. For example, making tasks you do all the time easier (cleaning data, summary), dealing with data structures that aren‚Äôt are easy to deal with (factors, strings.. etc), visualizations, and so much more."
  },
  {
    "objectID": "posts/twitters_favorite_lesser_known_packages/index.html#data-tasks",
    "href": "posts/twitters_favorite_lesser_known_packages/index.html#data-tasks",
    "title": "Twitter‚Äôs Favorite Lesser Known Packages",
    "section": "Data Tasks",
    "text": "Data Tasks\nMy favorite lesser known package is Janitor by Sam Firke. This package has basic functions to clean and prep messy data files. The functions are mostly relatively easy to replicate with dplyr, but why write the same thing over and over when Janitor does it for you!\n\n\nMine are, from janitor‚Ä¶1. clean_names2. get_dupes3. remove_empty#rstats\n\n‚Äî Erin Grand (@astroeringrand) December 11, 2018\n\n\nSkimr, as suggested by Fernando Flores, started at an ROpenSci Un-conf that provides a better summary function. It creates both a tidy version of the summary table to work with and a visual version to inspect. This is super useful for investigating data issues.\n\n\nCouldn‚Äôt choose just one package, so here we go:skimr::skimcovr::reportDT::JS\n\n‚Äî Fernando Flores (@ds_floresf) December 11, 2018"
  },
  {
    "objectID": "posts/twitters_favorite_lesser_known_packages/index.html#data-types",
    "href": "posts/twitters_favorite_lesser_known_packages/index.html#data-types",
    "title": "Twitter‚Äôs Favorite Lesser Known Packages",
    "section": "Data Types",
    "text": "Data Types\nThe tidyverse packages for dealing with specific data types are not nearly as widely used as they can be; forcats, lubridate, glue, and stringr can help solve so many problems with factor, dates, and strings.\n\n\nFrom forcats:1. fct_infreq2. fct_rev3. fct_drop\n\n‚Äî Emily Zabor (@zabormetrics) December 14, 2018\n\n\n\n\nforcats::fct_lumphttps://t.co/2BboLbdzuSglue::glue and glue::glue_datahttps://t.co/Bxt20MQGi2Cheated and use 2x packages.\n\n‚Äî Thomas Mock üë®üèº üíª (@thomas_mock) December 11, 2018\n\n\n\n\nyou stole mine! üòâ this is kind of cheating but from lubridate: year(), month(), day()\n\n‚Äî Luuuda (@ludmila_janda) December 11, 2018"
  },
  {
    "objectID": "posts/twitters_favorite_lesser_known_packages/index.html#plotting-support",
    "href": "posts/twitters_favorite_lesser_known_packages/index.html#plotting-support",
    "title": "Twitter‚Äôs Favorite Lesser Known Packages",
    "section": "Plotting Support",
    "text": "Plotting Support\nA few of the recommendations focused on vizulations and plotting. Key shouts outs for naniar and patchwork. Naniar helps you visualize your missing values. Patchwork allows you to combine plots together.\n\n\nFrom two packages, super handy at first steps after loading dataset: naniar::gg_miss_varsummarytools::descrsummarytools::freq\n\n‚Äî Radoslaw Panczak (@RPanczak) December 12, 2018"
  },
  {
    "objectID": "posts/twitters_favorite_lesser_known_packages/index.html#other",
    "href": "posts/twitters_favorite_lesser_known_packages/index.html#other",
    "title": "Twitter‚Äôs Favorite Lesser Known Packages",
    "section": "Other",
    "text": "Other\nThere are were a ton of other amazing offerings for excellent packages.\nThe magrittr package has many useful operators outside of the normal %&gt;% pipe.\n\n\nI was going to say %&lt;&gt;% , %&lt;&gt;% , and %&lt;&gt;% from magrittr - I use it all the time now thanks to @robinson_es - but now I‚Äôm browsing other magrittr functions and the aliases like extract() etc would be v handy when piping\n\n‚Äî Sarah R (@srhrnkn) December 12, 2018\n\n\nIf you work with spatial data at all, the sf package is a must.\n\n\nThe sf package cleared my skin, cleaned my home & cured my anxiety\n\n‚Äî Brooke Watson (@brookLYNevery1) December 11, 2018\n\n\n\n\nI added the conflicted package to my RProfile this summer, and I really love that it warns me about possible name conflicts before I run into problems pic.twitter.com/46Y88gexP9\n\n‚Äî Irene Steves (@i_steves) January 25, 2019\n\n\nWhat is your favorite lesser know package or function? Sound off in the comments (or find me on twitter)."
  },
  {
    "objectID": "posts/text_analysis_of_the_lizzie_bennet_diaries/index.html",
    "href": "posts/text_analysis_of_the_lizzie_bennet_diaries/index.html",
    "title": "Text Analysis of The Lizzie Bennet Diaries",
    "section": "",
    "text": "Inspired by Julia‚Äôs Silge‚Äôs recent talk on Tidytext as part of NASA Datanauts, and her blog posts, I decided to try my hand at some text analysis. Julia‚Äôs examples focus on the works of Jane Austen. As Jane Austen has been adapted so many time, I decided to ‚Äúadapt‚Äù Julia‚Äôs talk for the modern works of Austen‚Äôs book Pride and Prejudice - specifically the Lizzie Bennet Diaries.\nImage source: Pemberly Digital"
  },
  {
    "objectID": "posts/text_analysis_of_the_lizzie_bennet_diaries/index.html#gathering-data",
    "href": "posts/text_analysis_of_the_lizzie_bennet_diaries/index.html#gathering-data",
    "title": "Text Analysis of The Lizzie Bennet Diaries",
    "section": "Gathering Data",
    "text": "Gathering Data\nThe first part of this analysis is grabbing all the text from YouTube. To access the API, I use the tuber package by Gaurav Sood.\n\nlibrary(tidyverse)\nlibrary(tuber)\n\nyt_oauth(app_id, app_password)\nThe fist step was to find the channel id to access the LBD playlist. I do a quick search for lizziebennet to find some videos that I know are part of the series.\n\nsearch &lt;- yt_search(\"lizziebennet\")[1:5, ] \nsearch %&gt;% select(title, channelId)\n\n##                                                       title\n## 1                         My Name is Lizzie Bennet  - Ep: 1\n## 2 The Lizzie Bennet Diaries - Epis√≥dio 98 (LEGENDADO PT-BR)\n## 3                                      Yeah I Know - Ep: 61\n## 4                                 Introducing Lizzie Bennet\n## 5                                  The Lizzie Trap - Ep: 78\n##                  channelId\n## 1 UCXfbQAimgtbk4RAUHtIAUww\n## 2 UCfhdE-vIhW9GD0eGdd300ag\n## 3 UCXfbQAimgtbk4RAUHtIAUww\n## 4 UCGaVdbSav8xWuFWTadK6loA\n## 5 UCXfbQAimgtbk4RAUHtIAUww\nWith the channel ID in hand, I can now access the channel‚Äôs resources to find the playlist ID, which I will use to access all the videos in that playlist. list_channel_resources for tuber creates a list of channel attributes and buried in that list in the playlist ID.\n\n# Channel Information\na &lt;- list_channel_resources(filter = c(channel_id=\"UCXfbQAimgtbk4RAUHtIAUww\"), part=\"contentDetails\")\n\n# Uploaded playlists:\nplaylist_id &lt;- a$items[[1]]$contentDetails$relatedPlaylists$uploads\n\nplaylist_id\n\n## [1] \"UUXfbQAimgtbk4RAUHtIAUww\"\nThe YouTube API automatically pages videos so the max you get per page is 50. I know I need more than that, so I created a function that I call a few times to get all the videos. (This way works, but I would love any comments on how to make it better.)\n\n# pass NA as next page to get first page\nnextPageToken &lt;- NA\nvid_info &lt;-{}\n\n# Loop over every available page\nrepeat {\n  vids &lt;- get_playlist_items(filter= c(playlist_id=playlist_id), page_token = nextPageToken)\n  vid_ids &lt;- map(vids$items, \"contentDetails\") %&gt;%\n      map(\"videoId\")  %&gt;%\n      unlist()\n    \n  vid_info &lt;- vid_info %&gt;% bind_rows(tibble(ids = vid_ids))\n    \n  # get the token for the next page\n  nextPageToken &lt;- ifelse(!is.null(vids$nextPageToken), vids$nextPageToken, NA)\n    \n  # if no more pages then done\n  if(is.na(nextPageToken)){\n     break\n  }\n\n}\n\n# check that I have all 112 videos\nnrow(vid_info)\n\n## [1] 112\nNow that I have a list of video IDs, I can use get_captions to access the text of the videos. I also use xmlTreeParse and xmlToList to covert the caption into into an easily accessible lines of text. I put the text, video ID, and video title in a tibble for use in tidydata.\n\nlibrary(XML)\n\ngetText &lt;- function(id){\n  x &lt;- get_captions(id, lang = \"en\")\n  title &lt;- get_video_details(id)$title\n  a &lt;- xmlTreeParse(x)\n  text &lt;- a$doc$children$transcript\n  text &lt;- xmlToList(text, simplify = TRUE, addAttributes = FALSE) %&gt;% \n    tibble() %&gt;%\n    mutate(id = id, title = title)\n  return(text) \n}\n\ntext &lt;- map_df(vid_ids, getText) %&gt;% \n  set_names(c(\"text\", \"vid_id\", \"title\"))\nI don‚Äôt actually want to refer to each video by it‚Äôs full title, so I do some data munching to get each episode‚Äôs number (1-100). Notice, the Q&A videos do not get a episode number assigned to them. For the sake of this analysis, I‚Äôve decided to only work with the main 100 episodes.\n\ntitles &lt;- text %&gt;%\n  distinct(title) %&gt;%\n  mutate(title = ifelse(title == \"Question and Answers #3 (ft. Caroline Lee)\", \"Questions and Answers #3 (ft. Caroline Lee)\", title),\n         ep_num = gsub(\"[- .)(+!',/]|[a-zA-Z]*:?\",\"\", title),\n         ep_num = ifelse(title == \"2 + 1 - Ep: 73\", 73, ep_num),\n         ep_num = ifelse(title == \"25 Douchebags and a Gentleman - Ep:18\", 18, ep_num),\n         ep_num = ifelse(title == \"Bing Lee and His 500 Teenage Prostitutes - Ep: 4\", 4, ep_num),\n         ep_num = parse_number(ep.num)\n         ) %&gt;%\n  filter(!grepl(\"Questions and Answers\", title)) %&gt;%\n  arrange(ep_num) \nOne of the problems with using captions, is the messy text. I used a simple set of gsub commands to transform obvious punctuation marks into their English counterparts. I also pulled out the character SPEAKING the words from the text itself. I left this column alone in the data set, but might one day go back and focus an analysis on speaking characters.\n\nlibrary(tidytext)\nlibrary(stringr)\n\nlizziebennet &lt;- text %&gt;%\n  left_join(titles, by=\"title\") %&gt;%\n  filter(!is.na(ep_num)) %&gt;%\n  arrange(ep_num) %&gt;%\n  mutate(linenumber = row_number()) %&gt;%\n  mutate(text = gsub(\"&#39;\", \"'\", text),\n         text = gsub(\"&quot;\", '\\\"', text),\n         text = gsub(\"&amp;\", \"and\", text),\n         character = str_extract(text, \"^[a-zA-Z]*:\"),\n         text = sub(\"^[a-zA-Z]*:\", \"\", text)\n         ) %&gt;%\n  arrange(ep_num, linenumber)\nOkay, so now the text is mostly in place. The first thing I did was look at word counts. The most common words are not surprising, it‚Äôs just a list of the characters.\n\nlizziebennet %&gt;%\n  tidytext::unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words, by=\"word\") %&gt;%\n  count(word, sort=TRUE) %&gt;%\n  top_n(10)\n\n## # A tibble: 10 √ó 2\n##         word     n\n##        &lt;chr&gt; &lt;int&gt;\n## 1     lizzie   460\n## 2       jane   301\n## 3      darcy   243\n## 4       bing   232\n## 5    collins   220\n## 6      lydia   196\n## 7     bennet   194\n## 8  charlotte   180\n## 9       yeah   178\n## 10      time   176\nNot surprisingly, the most common trigrams are from the phrase that begins every episode, ‚ÄúMy name is Lizzie Bennet and‚Ä¶‚Äù\n\nlizziebennet %&gt;%\n  tidytext::unnest_tokens(word, text, token=\"ngrams\", n=3) %&gt;%\n  count(word, sort=TRUE) %&gt;%\n  top_n(10)\n\n## # A tibble: 10 √ó 2\n##                 word     n\n##                &lt;chr&gt; &lt;int&gt;\n## 1         my name is   106\n## 2   is lizzie bennet    96\n## 3     name is lizzie    96\n## 4  lizzie bennet and    84\n## 5       i don't know    40\n## 6          oh my god    36\n## 7           a lot of    33\n## 8        going to be    31\n## 9       what are you    29\n## 10     mr collins oh    28\nI was also especially amused by So good to see you! and THE MOST AWKWARD DANCE EVER being in the Top 10 5-grams.\n## # A tibble: 12 √ó 2\n##                           word     n\n##                          &lt;chr&gt; &lt;int&gt;\n## 1     my name is lizzie bennet    95\n## 2    name is lizzie bennet and    83\n## 3       is lizzie bennet and i    19\n## 4    is lizzie bennet and this    14\n## 5    lizzie bennet and this is    11\n## 6           so good to see you     9\n## 7       had nothing to do with     5\n## 8     is lizzie bennet and i'm     5\n## 9       lizzie bennet and i am     5\n## 10 the most awkward dance ever     5\n## 11     what are you doing here     5"
  },
  {
    "objectID": "posts/text_analysis_of_the_lizzie_bennet_diaries/index.html#sentiment-analysis",
    "href": "posts/text_analysis_of_the_lizzie_bennet_diaries/index.html#sentiment-analysis",
    "title": "Text Analysis of The Lizzie Bennet Diaries",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nI‚Äôve chosen to use the Bing lexicon (because of Bing Lee, get it?). In Tidydata, sentiment analysis is easy because you just join the lexicon against your tokenzied words.\n\nbing &lt;- sentiments %&gt;%\n        filter(lexicon == \"bing\") %&gt;%\n        select(-score)\n\nlbwordcount &lt;- lizziebennet %&gt;%\n  tidytext::unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(title)\n  \nlbsentiment &lt;- lizziebennet %&gt;%\n  tidytext::unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words) %&gt;%\n  inner_join(bing) %&gt;% \n  count(title, index=ep_num, sentiment) %&gt;% \n  spread(sentiment, n, fill = 0) %&gt;% \n  left_join(lbwordcount) %&gt;%\n  mutate(sentiment = positive - negative,\n         sentiment = sentiment / n)  \nMost positive sentiment episodes:\n## # A tibble: 5 √ó 2\n##                                       title  sentiment\n##                                       &lt;chr&gt;      &lt;dbl&gt;\n## 1                    Care Packages - Ep: 58 0.09623431\n## 2                         The End - Ep: 100 0.09375000\n## 3                   Jane Chimes In - Ep: 12 0.09132420\n## 4 My Parents: Opposingly Supportive - Ep: 3 0.08415842\n## 5      Wishing Something Universal - Ep: 76 0.08018868\nMost negative sentiment episodes:\n## # A tibble: 5 √ó 2\n##                            title   sentiment\n##                            &lt;chr&gt;       &lt;dbl&gt;\n## 1   Turn About the Room - Ep: 32 -0.15217391\n## 2        How About That - Ep: 91 -0.09937888\n## 3          Staff Spirit - Ep: 59 -0.09745763\n## 4 How to Hold a Grudge  - Ep: 74 -0.09352518\n## 5      Meeting Bing Lee - Ep: 28 -0.07614213\nThe next step was to visualize this in a way where you can look at the sentiment over the episodes.\n\nlibrary(viridis)\ntheme_set(theme_bw()) # a theme with a white background\n\nggplot(lbsentiment, aes(x=index, sentiment, fill=as.factor(index))) +\n        geom_bar(stat = \"identity\", show.legend = FALSE) +\n        theme_minimal(base_size = 13) +\n        geom_text(data=plot_text, aes(x=index, y=sentiment, label=index), size=3.5) + \n        labs(title = \"Sentiment in Lizzie Bennet Diaries\",\n             y = \"Sentiment\") +\n        scale_fill_viridis(end = 0.75, discrete=TRUE, direction = -1) +\n        scale_x_discrete(expand=c(0.02,0)) +\n        theme(strip.text=element_text(hjust=0)) +\n        theme(strip.text=element_text(face = \"italic\")) +\n        theme(axis.title.x=element_blank()) +\n        theme(axis.ticks.x=element_blank()) +\n        theme(axis.text.x=element_blank())\n Sentiment by main episode of LBD.\nJulia‚Äôs sentiment analysis of the original text is much more positive than my LBD analysis, with two negative portions relating to Darcy proposing to Elizabeth and Lydia running away with Wickham. I had expected a similar ‚ÄúWickham‚Äù negative spike in this plot, and while the section of Wickham related episodes (Ep 84 to Ep 89) is surely negative it‚Äôs not more negative than some of the introductory episodes.\nOne could argue, that since most of the Lydia - Wickham story line happens off screen and in Lydia‚Äôs blogs, that would explain that lack of a clear negative spike in the Wickham episodes."
  },
  {
    "objectID": "posts/text_analysis_of_the_lizzie_bennet_diaries/index.html#more-sentiment",
    "href": "posts/text_analysis_of_the_lizzie_bennet_diaries/index.html#more-sentiment",
    "title": "Text Analysis of The Lizzie Bennet Diaries",
    "section": "More sentiment",
    "text": "More sentiment\nContinuing the analysis, I wanted to look at which words were causing the largest effect on the overall sentiment.\n\nbing_word_counts %&gt;%\n  group_by(sentiment) %&gt;%\n  top_n(10) %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(word, n, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(y = \"Contribution to sentiment\",\n       x = NULL) +\n  coord_flip()\n\nGiven that this is a modern adaption, it‚Äôs interesting that much like the analysis done on the original ‚Äúmiss‚Äù is the top contribution to negative sentiment. In the original text I would assume a higher count of ‚ÄúMiss Bennet‚Äôs‚Äù to the modernized version. However, Lizzie does talk about you she‚Äôll miss Charlotte, or she misses her home‚Ä¶ etc, so it‚Äôs not too surprising to see it have a considerable contribution here.\nI did a bit of an investigation into this with bigrams.\n\nlizziebennet %&gt;%\n  tidytext::unnest_tokens(bigram, text, token=\"ngrams\", n=2) %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %&gt;%\n  filter(word1 == \"miss\") %&gt;%\n  mutate(miss_in_name = ifelse(word2 %in% c(\"bennet\", \"lu\"), \"Yes\", \"No\")) %&gt;%\n  count(miss_in_name)\n\n## # A tibble: 2 √ó 2\n##   miss_in_name     n\n##          &lt;chr&gt; &lt;int&gt;\n## 1           No    26\n## 2          Yes    27\nAnd oddly enough, the use of the word ‚Äúmiss‚Äù is about half and half between ‚ÄúI miss [person/thing]‚Äù and ‚ÄúMiss Bennet‚Äù type phrases. Interesting! (Anyone want to guess who refers to Lizzie as Miss Bennet the most? Unsurprisingly, it‚Äôs Ricky Collins.)"
  },
  {
    "objectID": "posts/text_analysis_of_the_lizzie_bennet_diaries/index.html#more-with-bigrams",
    "href": "posts/text_analysis_of_the_lizzie_bennet_diaries/index.html#more-with-bigrams",
    "title": "Text Analysis of The Lizzie Bennet Diaries",
    "section": "More with Bigrams",
    "text": "More with Bigrams\n\nbigrams_separated &lt;- lizziebennet %&gt;%\n  tidytext::unnest_tokens(bigram, text, token=\"ngrams\", n=2) %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %&gt;%\n  filter(!word1 %in% stop_words$word, \n         !word2 %in% stop_words$word) %&gt;%\n  count(word1, word2, sort = TRUE)\n\nbigrams_separated %&gt;% \n  ungroup() %&gt;%\n  top_n(10) \n\n## # A tibble: 11 √ó 3\n##      word1   word2     n\n##      &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;\n## 1   lizzie  bennet   132\n## 2     bing     lee    43\n## 3   george wickham    24\n## 4      hey  lizzie    24\n## 5       de  bourgh    22\n## 6    ricky collins    21\n## 7      los angeles    20\n## 8     miss  bennet    19\n## 9     tour  leader    18\n## 10   video    blog    17\n## 11 william   darcy    17\nNot surprisingly, the common bigrams are first and last names of characters, but there‚Äôs also some fun other popular bigrams with ‚Äútour leader‚Äù and ‚Äúvideo blog.‚Äù I guess vlog wasn‚Äôt super popular to use on it‚Äôs own yet."
  },
  {
    "objectID": "posts/text_analysis_of_the_lizzie_bennet_diaries/index.html#network-of-words",
    "href": "posts/text_analysis_of_the_lizzie_bennet_diaries/index.html#network-of-words",
    "title": "Text Analysis of The Lizzie Bennet Diaries",
    "section": "Network of Words",
    "text": "Network of Words\nOne of my favorite part of tidytext is the example on making a bigram network. It‚Äôs just so fun!\n\nlibrary(igraph)\nlibrary(ggraph)\n\nset.seed(42)\n\nbigrams_separated %&gt;%\n  filter(n &gt; 5) %&gt;%\n  graph_from_data_frame() %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\n  theme(axis.title.x=element_blank()) +\n  theme(axis.ticks.x=element_blank()) +\n  theme(axis.text.x=element_blank()) +\n  theme(axis.title.y=element_blank()) +\n  theme(axis.ticks.y=element_blank()) +\n  theme(axis.text.y=element_blank())\n\nI especially enjoy the Bennet sister cluster in the left corner.\n\nI leave you with this last picture.\n Some of cast of Lizzie Bennet Diaries and me. Vidcon 2014"
  },
  {
    "objectID": "posts/graphics_and_plots_in_science/index.html",
    "href": "posts/graphics_and_plots_in_science/index.html",
    "title": "Graphics in Science",
    "section": "",
    "text": "Graphics and visualizations are used for promotion, advertisement to promote a product or idea. In science, graphics tend to fall into one of two categories: for use in education or a science journal. For information on what makes a good educational graphic, or a teaching tool, I‚Äôve written a piece earlier on this blog here. In the academic articles, graphics hold a special role in telling a compelling story of the data and results, however, the editing emphasis is often placed much more on text than making interesting and understandable science graphics.\nDISCLAIMER: I am coming from an astronomy and physics background, and am going to discuss problems found within these contexts.\n\nAcademic Article Graphics\nWe think of academics and especially science as being told through plots and graphs. In fact, Tufte explains that the history graphics begins with time series plots of the planets and the sun in the night sky. Now a days, science articles use graphics to tell a story. We let the data speak for itself by representing it in a reproducible graphic.\nIn my time in academia (in physics and astronomy) I‚Äôve come across several common problems such as:\n\nMissing or incorrect error bars (especially on log-log plots)\nMissing or incorrect ticks marks and axis labels\nToo much text - Keep notes and explanations outside the graphic and in the image caption\nOverlap of lines or points\nWasting space or not using enough of it\nPlots that should have been tables\n\nSome of these problems come from trying to show off too much of the data. You want the data to stand out, but you don‚Äôt always need to include all of it. This is hard because we spend so much time working with the data that we want to share everything, but the added complexity often takes away from the graph and the point you‚Äôre trying to make.\nIn the remainder of the blog, I will try to address each of these points and introduce a fast and easy way to correct them.\n\nCorrections to common problem with academic graphics: * Log-log plots with missing or symmetric error bars can be fixed by forcing asymmetric error bars. When there are small errors, the log can show as a negative error, which often means that plot won‚Äôt do anything. In Matplotlib the default (for the y axis) is to map all negative values a very small positive one. The code for that is:\nplt.yscale('log', nonposx='clip')\n\nTick marks: The defaults for tick marks and labels are often too large, too small, facing the wrong direction or else wise strange. In ggplot in R this can be manipulated under theme:\n\nFor example, to make everything expect your points or lines disappear you‚Äôd use:\n theme(axis.line=element_blank(),\n        axis.text.x=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks=element_blank(),\n        axis.title.x=element_blank(),\n        axis.title.y=element_blank(),\n        legend.position=\"none\",\n        panel.background=element_blank(),\n        panel.border=element_blank(),\n        panel.grid.major=element_blank(),\n        panel.grid.minor=element_blank(),\n        plot.background=element_blank()) \nEach of these could also be modified to make the text larger or smaller, change the font, rotate the labels‚Ä¶etc.\nTo change the direction and size of the tick labels you‚Äôd use something like:\ntheme(axis.text.x  = element_text(angle=90, vjust=0.5, size=16)\n\nYou can reduce clutter on the graph by using fewer (labeled) tick marks.\nAlways remember to label your axes! This is done in python with:\n\nimport matplotlib.pyplot as plt\nplt.xlabel('X Label')\nplt.ylabel('Y Label')\nor in ggplot2 in R using:\n  labs(title = \"Plot Title\") +\n  ylab(\"X Label\") +\n  xlab(\"Y Label\")\n\nThe problem of too many overlapping lines or points can be solved in various ways depending on the data. Sometimes, changing the colors and alpha of the points might be enough. In other cases, it‚Äôs best to separate out the information into a table of plots.\n\nFor example, below is the original plot from my research showing the intensity of different molecules across velocities. The plot places all four molecules on the same graph with a key indicating which is which. In color, this graphic might make more sense, but it is still hard to make out the individual curves. Plus, the key is small and referring back to it is time consuming and annoying.\n\n\n\nVelocity spectra for the Pelican Pillar\n\n\nIn fixing the graphic, while also including more information from my other sources, I separated out the each of the molecules and sources into a table of spectra. This un-clutters the plot and allows you to more easily visualize trends in the sources. (Notice how the plot is missing axes labels - shame on me!)\n\n\n\nVelocity spectra for the pillars show brightness temperature against velocity in km/s. The spectra were taken in the heads of the pillars at the peak brightness and averaged over a beam size.\n\n\nThe code for this plot was done in IDL - a language mostly used only by astronomers (after looking at the code, you‚Äôll see why no one else joined in the fun‚Ä¶) If you‚Äôre interested, you can check it out here.\nFor the future, I want to try and remake some of my research plots in R for better practice with R and ggplot2, using something along these lines:\ndata %&gt;%\nggplot(aes(x=vel,y=tb)) + \ngeom_line() + \nfacet_wrap(~pillar)\n\nIn attempts to not waste space, you should examine the size and scale of the axes. This often shows up as a problem when an outlier or two that expand the axes such that much of the plot is empty. In these cases, you can crop the plot to the main data and include an arrow to show where the outlier is.\n\nMost importantly, don‚Äôt display empty plots like this infamous plot  from Wittke-Thompson JK and Pluzhnikov A, Cox NJ (2005).\n\nSometimes, to save room or otherwise, it‚Äôs best to display the information in a table format instead of bar graphs. For example, this plot from a science article titled ‚ÄúStudying Gender in Conference Talks ‚Äì data from the 223rd meeting of the American Astronomical Society‚Äù shows the large difference in number questions asked by males vs number asked by females, given a male or female chair. This plot displays the most significant finding from the analysis: a strong dependence on session chair gender. Still, this information could have easily been shown in a table instead of a graph. This would be a useful plot for a presentation on the subject, but not needed in the article.\n\n\n\n\n\n\nColor in academic graphics:\nColor can be a huge issue in scientific articles. This is largely because most journals charge more for printing in color, but will present colored versions of plots in the online versions on the articles. This means that authors need to make sure that they have plots that work well in color and in black and white, which gives way to some graphics which are very hard to read.\n\nCommon color problems.\n\nEye piercing bight colors and/or use of rainbow colors. We‚Äôve discussed the problems with the rainbow in class, but as a reminder: the rainbow color scheme includes colors which are hard to see, doesn‚Äôt have a universally understood order, artificially exaggerates differences in color while softening the differences between others and (importantly for print) doesn‚Äôt convert well to black and white.\n\nContrast is one of THE biggest problems I see in academic figures. Things like cyan or yellow on white, red on blue, navy on black‚Ä¶ these cause major problems (and headaches) when reading text or trying to discern between lines. Your plot doesn‚Äôt have to be pretty, but it does have to be legible!\nColor in astronomy maps often tags along with Color-coded image of the molecular cloud\n\n\n\n\nGraph Critique and Fix\n\nThe image from an article titled ‚ÄúMOLECULAR CLOUDS IN THE NORTH AMERICAN AND PELICAN NEBULAE: STRUCTURES‚Äù by Shaobo Zhang, Ye Xu and Ji Yang, displays the locations of clumps, as well as their velocity and size. From the image caption ‚ÄúThe circles indicate the clump positions on the integrated intensity map of 13 CO. The colors of the circles represent the velocities the clumps, while the circles are scaled according to the sizes of clumps.‚Äù\nThis is a perfect example of trying to show too much in one plot such that it‚Äôs no longer understandable. A different color scheme would help the eye more easily see the trends in velocity. I would also like to see the circles filled in, and the background map a bit darker. Also, The graph extends too far up so that the color legend is clear, but leaves too much empty space in the graph. The axes and tick marks could also be smaller.\nI didn‚Äôt have their data, but I remade a similar type of plot pulling points, velocities and sizes from normal distributions (see code below).\n\nThis image fixes some of the problem by using GGPLOT default color scheme, which keeps the hue in blue and changes the brightness. I‚Äôve filled in the circle in make the difference in sizes more clear, and I made sure that the circles are scaled by area, as to not conflate radius and area.\nlibrary(ggplot2) \n\nxvar &lt;- sample(-100:100,100,replace=T)\nyvar &lt;- sample(-100:100,100,replace=T)\nv &lt;-  sort(rnorm(100,0,1))\ns &lt;- abs(rnorm(100))\n\ndata &lt;- data.frame(xvar,yvar,v,s)\ndata &lt;- data[order(data$v),]\ndata$x &lt;- rnorm(100,0,50)\ndata$y &lt;- rnorm(100,0,50)\n\nggplot(data,aes(x=x,y=y), legend=FALSE)+\n  stat_density2d(aes(alpha=..level..), geom=\"polygon\", show_guide=FALSE) +\n  scale_alpha_continuous(limits=c(-2,10), breaks=seq(0,1,by=1)) + \n  geom_point(aes(x=xvar, y=yvar, size=s, color=v), alpha=0.8, show_guide=FALSE) + \n  scale_colour_gradient(limits=c(-4,4)) +\n  scale_size_area(max_size=10)   + \n  theme_bw() + \n  theme(legend.title=element_blank()) \n\n\n\nConclusions:\n* Always remember to think about the story your telling and how your graphic fits in. * Label your plots correctly, but don‚Äôt clog the plot with text. Keep your labels short, and rotate them if needed to to be read. * If displaying all of your data looks cluttered, think about if you really need to show all of it, and if so if there‚Äôs a better way to display it. * Watch out for color! We like pretty graphs, but only if we can still read them."
  },
  {
    "objectID": "posts/graphics_edu_in_science/index.html",
    "href": "posts/graphics_edu_in_science/index.html",
    "title": "Educational Graphics in Science",
    "section": "",
    "text": "Educational graphics are often used as a way to teach one concept. As such, they tend to generalize the information in such a way that leaves out important information.\nTake, for example the standard evolution depiction:\n\n\n\nEvolution image\n\n\nWe use this image as a teaching tool, because it‚Äôs easy to visual evolution this way. However, the graphic, isn‚Äôt accurate in it‚Äôs depiction of evolution because due to the simplicity of the graphic there is no knowledge of time, species extinction, A better graphic would be able to easily show all of this information.\nThe image below does a little better by including time information as well as showing the full evolution tree. This graphic makes clear the complex nature of evolution, but still leaves out many extinction events. (Click  here  to see a larger version of this image.) \n\nFor an example of commonly used well depicted graphic, we have this image showing the structure of the terrestrial planets.\n\nThis graphic shows the relative size difference between the planets as well as their inside makeup. It even manages to show where we have incomplete information. There are lots of version of this graphic, but I think this one works the best because it uses labels and lines sparingly in place of a color coded legend."
  },
  {
    "objectID": "posts/graphics_edu_in_science/index.html#educational-graphics",
    "href": "posts/graphics_edu_in_science/index.html#educational-graphics",
    "title": "Educational Graphics in Science",
    "section": "",
    "text": "Educational graphics are often used as a way to teach one concept. As such, they tend to generalize the information in such a way that leaves out important information.\nTake, for example the standard evolution depiction:\n\n\n\nEvolution image\n\n\nWe use this image as a teaching tool, because it‚Äôs easy to visual evolution this way. However, the graphic, isn‚Äôt accurate in it‚Äôs depiction of evolution because due to the simplicity of the graphic there is no knowledge of time, species extinction, A better graphic would be able to easily show all of this information.\nThe image below does a little better by including time information as well as showing the full evolution tree. This graphic makes clear the complex nature of evolution, but still leaves out many extinction events. (Click  here  to see a larger version of this image.) \n\nFor an example of commonly used well depicted graphic, we have this image showing the structure of the terrestrial planets.\n\nThis graphic shows the relative size difference between the planets as well as their inside makeup. It even manages to show where we have incomplete information. There are lots of version of this graphic, but I think this one works the best because it uses labels and lines sparingly in place of a color coded legend."
  },
  {
    "objectID": "posts/cleaning_data_in_r/index.html",
    "href": "posts/cleaning_data_in_r/index.html",
    "title": "Best Practices for Cleaning Data in R",
    "section": "",
    "text": "A few months ago, I gave a talk at the previously known as NYC R conference, now known as New York Data Science and AI. (Watch it here!) My presentation focused on my favorite topic: handling duplicates in data, and the importance of data cleaning.\nThe saying ‚Äú90% of data science is cleaning the data‚Äù rings especially true for me. I love really love digging into the weeds of cleaning data - figuring out what went wrong, whether the errors were systematic or not, whether there were user input errors (always), etc.\nI‚Äôve spent the last 10+ years working in the education space, where messy data is everywhere and data-driven decisions have a real impact on a student‚Äôs success. The challenge is to get data as clean as possible to have a correct analysis to base the decisions on.\nSome common data challenges I‚Äôve seen are:\nTackling these and more messy data challenges is the 90% of the work that drives meaningful outcomes for students."
  },
  {
    "objectID": "posts/cleaning_data_in_r/index.html#duplicates-oh-no-where-did-they-come-from",
    "href": "posts/cleaning_data_in_r/index.html#duplicates-oh-no-where-did-they-come-from",
    "title": "Best Practices for Cleaning Data in R",
    "section": "Duplicates! Oh no! Where did they come from?",
    "text": "Duplicates! Oh no! Where did they come from?\nDuplicates in data are everywhere. Any dataset has the potential for some level of duplication, and if you‚Äôre not on the lookout, they can persist and cause analysis errors.\nMost data duplicates that I‚Äôve seen are caused by inadequate processes. If your organization doesn‚Äôt have the right data processes to start with, messy data will continue to flow, no matter how much you code. Creating and training in structures and processes will help reduce errors across the board.\nFor example, let‚Äôs say we have a student named James, who moved from School A to School B mid-year.\nBad process example: School B records James entering the school the week before he officially starts, to get started on his course schedule and other paperwork. School A doesn‚Äôt record his exit until a week after he left, because they got busy or wanted to wait to see if he changed his mind. As the data person, you don‚Äôt know which school James actually attended during the overlapping two weeks in the database.\nBetter process example: Make sure the system of record allows forward and back dating such that School B records James in their system with the correct start date and School A records James as having left on his last day. If James comes back to School A, they will start a new record without overlapping dates. To ensure data uniqueness, the system should verify that James has the same ID in School A and School B.\nThis example and other data issues can be checked and rechecked through data audits. Even with identifiers, duplicates can still occur (e.g., the same person with two different email addresses), so we use additional fields to audit for duplicates. Names, emails, birthdays, phone numbers, and home addresses are good places to check\n\nDuplicates you caused!\nDuplicates are not always the fault of the data itself. We can cause our own duplicates through incorrectly written code.\nJoining ‚Äì using the incorrect fields or edits to fields needed pivot_wider ‚Äì including too many columns in the select pivot_longer ‚Äì including too many columns in the pivot Integrating validation steps, unit testing, and code reviews into your work will reduce the number of ‚Äúcoder-caused‚Äù duplicates."
  },
  {
    "objectID": "posts/cleaning_data_in_r/index.html#lets-take-a-look-at-some-r-code",
    "href": "posts/cleaning_data_in_r/index.html#lets-take-a-look-at-some-r-code",
    "title": "Best Practices for Cleaning Data in R",
    "section": "Let‚Äôs take a look at some R code‚Ä¶",
    "text": "Let‚Äôs take a look at some R code‚Ä¶\n\nJanitor was built with beginning-to-intermediate R users in mind and is optimized for user-friendliness. Advanced users can already do everything covered here, but they can do it faster with Janitor and save their thinking for more fun tasks. (Sam Firke)\n\nIf you‚Äôre experienced with Tidyverse in general, you should be able to do everything inside Janitor on your own; however, it‚Äôs always nice to have a function do it for you.\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(readxl)\n\n# Set up fake student data\n# Your fake data might be different from mine, as it's totally random IDs.\nstudents &lt;- tibble::tibble(student_id = round(runif(10, 1e6, 1e7-1), 0), \n                           grade = round(runif(10, 1, 12)),\n                           entrydate = Sys.Date() - 30,\n                           exitdate = Sys.Date())\n\nstudents[3, 1]  &lt;- students[5, 1] \nstudents[3, 2]  &lt;- students[5, 2] - 1 # set up duplicate\n\nstudents |&gt;  get_dupes(student_id)\n# A tibble: 2 x 6\n  student_id dupe_count grade  entrydate        exitdate\n       &lt;dbl&gt;      &lt;int&gt;  &lt;dbl&gt;          &lt;date&gt;            &lt;date&gt;\n1    4137115          2         1             2017-12-02    2018-01-01\n2    4137115          2         2            2017-12-02    2018-01-01\nUsing get_dupes and verify() from the assertr package is a great way to add checks in case the data changes (which it inevitably will).\ncheck &lt;- students |&gt; \n  get_dupes(student_id) |&gt; \n  verify(nrow(.) == 0)\nIf a student ID changes or new duplicates occur, the code will stop at this step.\n\nFixing the duplicates\nOption 1:\nCorrect the dupes individually with if_else or case_when. This method is best for errors in one or 2 rows as a quick and easy fix.\ncorrect_students &lt;- students |&gt;\n  mutate(grade = if_else(student_id == ______, CORRECT-GRADE, grade)) |&gt;\n  distinct() \nOption 2:\nSystematic errors can be fixed by taking a summarize on the incorrect column. In this case, we could assume that the lower grade-level is the correct one for all duplicate enrollments. This method works better for systematic issues that you know how to correct, such as taking the higher of two homework assignemtns.\ncorrect_students &lt;- students |&gt;\n   group_by(student_id) |&gt; \n   summarize(grade = min(grade))\n\ncorrect_students\nOption 3:\nOutput the duplicates and manually choose which version to keep.\nstudents |&gt; \n  get_dupes(student_id) |&gt;\n  write_csv(\"../data/dupes.csv\", na = \"\")\n\n# Create this file in excel manually, often by asking your coworkers for help\ndupes_remove &lt;- read_csv(\"../data/dupes_correct.csv\") |&gt; \n  filter(delete == 1)\n\nstudents_correct &lt;- anti_join(students, dupes_remove) \n\n\nDOCUMENT DOCUMENT DOCUMENT.\nNow that you‚Äôve fixed the duplicates, whether in the database and/or code, DOCUMENT what you did and WHY, so that when the data changes and new duplicates are found, the code still runs."
  },
  {
    "objectID": "posts/scraping_apod_descriptions/index.html",
    "href": "posts/scraping_apod_descriptions/index.html",
    "title": "Scraping APOD Descriptions",
    "section": "",
    "text": "A long while ago now, Nujchare tweeted about an awesome vis she did using rvest and PowerBi.\n\n\nUsing rvest + purrr packages to scrap APOD. PowerBI viz it up real nice! #rstats #powerbi. My learning journal during #NASADatanauts year of awesomeness. https://t.co/cnwttLPoIS pic.twitter.com/je511h99L9\n\n‚Äî Nujcharee (‡πÄ‡∏õ‡πá‡∏î) (@Nujcharee) December 8, 2017\n\n\nI complemented her work and she asked me to look over the code. I jumped at the chance, (1) because I don‚Äôt know a ton about scraping website data and wanted to see what she started, (2) I could help with the dplyr part of the code, and most importantly (3) I love APOD!\nI love APOD so much, that for most of my childhood my life goal was ‚Äúget a picture published to APOD.‚Äù To make matters more exciting in 2009 this happened."
  },
  {
    "objectID": "posts/scraping_apod_descriptions/index.html#orginal-plan---scrape-from-archive",
    "href": "posts/scraping_apod_descriptions/index.html#orginal-plan---scrape-from-archive",
    "title": "Scraping APOD Descriptions",
    "section": "",
    "text": "A long while ago now, Nujchare tweeted about an awesome vis she did using rvest and PowerBi.\n\n\nUsing rvest + purrr packages to scrap APOD. PowerBI viz it up real nice! #rstats #powerbi. My learning journal during #NASADatanauts year of awesomeness. https://t.co/cnwttLPoIS pic.twitter.com/je511h99L9\n\n‚Äî Nujcharee (‡πÄ‡∏õ‡πá‡∏î) (@Nujcharee) December 8, 2017\n\n\nI complemented her work and she asked me to look over the code. I jumped at the chance, (1) because I don‚Äôt know a ton about scraping website data and wanted to see what she started, (2) I could help with the dplyr part of the code, and most importantly (3) I love APOD!\nI love APOD so much, that for most of my childhood my life goal was ‚Äúget a picture published to APOD.‚Äù To make matters more exciting in 2009 this happened."
  },
  {
    "objectID": "posts/scraping_apod_descriptions/index.html#getting-the-data",
    "href": "posts/scraping_apod_descriptions/index.html#getting-the-data",
    "title": "Scraping APOD Descriptions",
    "section": "Getting the Data",
    "text": "Getting the Data\nTo start, we grab the information from the landing page of APOD‚Äôs archive and ignore any links that are not pictures of the day. (Luckily, these all start with ‚Äúap‚Äù so we can use str_detect() to find them.)\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(tidytext)\n\n## scrape the landing page\napod &lt;- read_html(\"https://apod.nasa.gov/apod/archivepix.html\")\n\n## scrape all URLs\nurl &lt;- html_nodes(apod, \"a\") %&gt;% \n  map_chr(xml_attrs) %&gt;%\n  tibble(url = .) %&gt;%\n  filter(str_detect(url, \"ap\"), !str_detect(url, \"/\"))\nNext, we have to go to each of the pages and scrape the underlying page data. There are A LOT of APODs, so this can take a long time. I‚Äôve chosen to only look at the first 1000 images for now. (More on solving this at the end!)\n# read html from url\nmy_read_html &lt;- function(url, ...) {\n  xml2::read_html(url, ...)\n}\n\ndata_raw &lt;- url[1:1000, ] %&gt;% # first 1000 links\n  mutate(full_url = paste0(\"https://apod.nasa.gov/apod/\", url)) %&gt;%\n  mutate(page = map(full_url, my_read_html),\n         pic = map_chr(page, ~html_node(.x, xpath = \"//*/img\") %&gt;% html_attr(\"src\")),\n         title = map_chr(page, ~html_nodes(.x, \"title\") %&gt;% html_text()),\n         description = map_chr(page, ~html_nodes(.x, \"p\") %&gt;% html_text() %&gt;% .[str_detect(., \"Ex\")]) # descriptions start with \"Explanation:\"\n         )"
  },
  {
    "objectID": "posts/scraping_apod_descriptions/index.html#data-cleaning",
    "href": "posts/scraping_apod_descriptions/index.html#data-cleaning",
    "title": "Scraping APOD Descriptions",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nWith the raw data in hand, I move into more specific text cleaning. I want to start with some quick tidy text analysis of the descriptions, so I cant to clean that up first.\ndata &lt;- data_raw %&gt;% \n  mutate(description = str_replace_all(description, \"\\n\", \" \"),\n         description = str_replace_all(description, \"Explanation:\", \"\"),\n         title = str_replace_all(title, \"\\n\", \" \"),\n         title = str_replace_all(title, \"APOD:\", \"\"),\n         title = if_else(str_detect(title, \"2017 November 22\"), \"2017 November 22 - Oumuamua Interstellar Asteroid\", title)\n         ) %&gt;%\n  mutate_all(trimws) %&gt;%\n  separate(title, into = c(\"date\", \"title\"), sep = \" - \")\nGreat, now we can a do a quick word count using tidytext tools."
  },
  {
    "objectID": "posts/scraping_apod_descriptions/index.html#fun-stuff---word-count",
    "href": "posts/scraping_apod_descriptions/index.html#fun-stuff---word-count",
    "title": "Scraping APOD Descriptions",
    "section": "Fun Stuff - word count",
    "text": "Fun Stuff - word count\nkeep_words &lt;- c(\"way\") # I don't want \"way\" as in \"Milky Way\" to be filtered\nmy_stop_words &lt;- tibble(word = c(\"image\")) %&gt;%\n  mutate(lexicon = \"PERSONAL\") %&gt;%\n  bind_rows(stop_words) %&gt;%\n  filter(!word %in% keep_words)\n  \n\ndata %&gt;%\n  select(-date) %&gt;%\n  distinct() %&gt;%\n  unnest_tokens(word, description) %&gt;%\n  anti_join(my_stop_words) %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  top_n(10)\n## # A tibble: 10 x 2\n##    word       n\n##    &lt;chr&gt;  &lt;int&gt;\n##  1 light   1088\n##  2 star     725\n##  3 stars    656\n##  4 galaxy   636\n##  5 nebula   627\n##  6 moon     522\n##  7 sun      497\n##  8 earth    496\n##  9 bright   461\n## 10 sky      411\nI love this because it clearly shows the types of objects that make up most of pretty Astronomy pictures, i.e stars, galaxies and nebulae. Very cool!\nIf I look at bi-grams is there any doubt that ‚ÄúMilky Way‚Äù will have a strong showing?\ndata %&gt;%\n  select(-date) %&gt;%\n  distinct() %&gt;%\n  unnest_tokens(word, description, token = \"ngrams\", n = 2) %&gt;%\n  select(title, word) %&gt;%\n  separate(word, into = c(\"word1\", \"word2\"), by = \" \") %&gt;%\n  unite(word, word1, word2, sep = \" \") %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  top_n(10)\n## # A tibble: 10 x 2\n##    word                n\n##    &lt;chr&gt;           &lt;int&gt;\n##  1 milky way         315\n##  2 planet earth      205\n##  3 way galaxy        133\n##  4 million light     118\n##  5 solar system      114\n##  6 space telescope   106\n##  7 star forming      100\n##  8 hubble space       88\n##  9 spiral galaxy      88\n## 10 star cluster       75\n‚Ä¶and there it is, clearly winning over ‚ÄúPlanet Earth‚Äù and ‚ÄúSolar System.‚Äù\nAs a person who studied star formation, I‚Äôm also proud of the strong showing of ‚Äústar forming‚Äù in the bi-grams. Yay baby stars!"
  },
  {
    "objectID": "posts/scraping_apod_descriptions/index.html#but-wait-isnt-there-an-api",
    "href": "posts/scraping_apod_descriptions/index.html#but-wait-isnt-there-an-api",
    "title": "Scraping APOD Descriptions",
    "section": "But wait‚Ä¶ isn‚Äôt there an API?",
    "text": "But wait‚Ä¶ isn‚Äôt there an API?\nThis is great and fun, but what I‚Äôd really love to look at the entire APOD archive, or pull a specific date. Luckily, NASA has a great API to do just that! The API is super easy to use and simple enough to write into some R functions. I decided the coolest thing to do with this API was create a package, and thus my new package - astropic was born (available on github)!\nThe goal of astropic is to connect R to the NASA APOD API. The APOD API supports one image at a time. In order to supply more than that, this package also includes creating time ranges (of less than 1000 days at a time) and some historical data in tibble format.\nYou can install the current version from GitHub to check it out\n# install.packages(\"devtools\")\ndevtools::install_github(\"eringrand/astropic\")\nAstropic does not yet contain ANY tests and the documentation is very sparse. It is most definitely a work in progress - I‚Äôll update more as I add more to it.\nNext time on the blog, more about the package creation and cool things you can do with it. In the mean time, please feel free to send pull requests and let me know what you‚Äôd like from such a package."
  },
  {
    "objectID": "posts/update_to_lizzie_bennet_text_analysis_using_plotly/index.html",
    "href": "posts/update_to_lizzie_bennet_text_analysis_using_plotly/index.html",
    "title": "Update to Lizzie Bennet Text Analysis Using Plotly",
    "section": "",
    "text": "Daniela V√°zquez recently published her blog post on Last Week Tonight. She used a bunch of code from my previous LBD analysis (THANKS FOR THE LOVE DANIELA! :heart:) and also created this super cool plotly widget.\nI had never used ggplotw2 and plotly before and wanted to give it a try, recreating a previous plot of sentiment by LBD episode.\nlibrary(viridis)\nlibrary(plotly)\n\np &lt;- ggplot(lbsentiment, aes(x=index, sentiment, fill=as.factor(index), text=title)) +\n    geom_bar(stat = \"identity\", show.legend = FALSE) +\n    theme_minimal(base_size = 13) +\n    geom_text(aes(x=index, y=plot_sentiment, label=plot_index), size=3.5) + \n    labs(title = \"Sentiment in Lizzie Bennet Diaries\",\n         y = \"Sentiment\"\n         ) +\n    scale_fill_viridis(end = 0.75, discrete=TRUE, direction = -1) +\n    scale_x_discrete(expand=c(0.02,0)) +\n    theme(strip.text=element_text(hjust=0)) +\n    theme(strip.text=element_text(face = \"italic\")) +\n    theme(axis.title.x=element_blank()) +\n    theme(axis.ticks.x=element_blank()) +\n    theme(axis.text.x=element_blank()) +\n    theme(legend.position = \"none\")\n\nggplotly(p, tooltip=\"text\", width=750, height=400)"
  }
]